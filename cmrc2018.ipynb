{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import paddlenlp as ppnlp\n",
    "from utils import prepare_train_features, prepare_validation_features\n",
    "from functools import partial\n",
    "from paddlenlp.metrics.squad import squad_evaluate, compute_prediction\n",
    "import paddle\n",
    "from paddlenlp.data import Stack, Dict, Pad\n",
    "import collections\n",
    "import time\n",
    "import json\n",
    "from paddlenlp.datasets import DatasetBuilder\n",
    "import inspect\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip -o data/data116454/cmrc2018_public.zip -d data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CMRC2018(DatasetBuilder):\n",
    "    '''\n",
    "    This dataset is a Span-Extraction dataset for Chinese machine reading \n",
    "    comprehension. The dataset is composed by near 20,000 real questions \n",
    "    annotated on Wikipedia paragraphs by human experts.\n",
    "    '''\n",
    "\n",
    "    SPLITS = {\n",
    "        'train': 'train.json',\n",
    "        'dev': 'dev.json',\n",
    "        'test': 'test.json'\n",
    "    }\n",
    "\n",
    "    def _get_data(self, mode, **kwargs):\n",
    "        default_root = 'data'\n",
    "        filename = self.SPLITS[mode]\n",
    "        fullname = os.path.join(default_root, filename)\n",
    "\n",
    "        return fullname\n",
    "\n",
    "    def _read(self, filename, *args):\n",
    "        with open(filename, \"r\", encoding=\"utf8\") as f:\n",
    "            input_data = json.load(f)[\"data\"]\n",
    "        for entry in input_data:\n",
    "            title = entry.get(\"title\", \"\").strip()\n",
    "            for paragraph in entry[\"paragraphs\"]:\n",
    "                context = paragraph[\"context\"].strip()\n",
    "                for qa in paragraph[\"qas\"]:\n",
    "                    qas_id = qa[\"id\"]\n",
    "                    question = qa[\"question\"].strip()\n",
    "                    answer_starts = [\n",
    "                        answer[\"answer_start\"]\n",
    "                        for answer in qa.get(\"answers\", [])\n",
    "                    ]\n",
    "                    answers = [\n",
    "                        answer[\"text\"].strip()\n",
    "                        for answer in qa.get(\"answers\", [])\n",
    "                    ]\n",
    "\n",
    "                    yield {\n",
    "                        'id': qas_id,\n",
    "                        'title': title,\n",
    "                        'context': context,\n",
    "                        'question': question,\n",
    "                        'answers': answers,\n",
    "                        'answer_starts': answer_starts\n",
    "                    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path_or_read_func,\n",
    "                 name=None,\n",
    "                 data_files=None,\n",
    "                 splits=None,\n",
    "                 lazy=None,\n",
    "                 **kwargs):\n",
    "   \n",
    "    reader_cls = CMRC2018\n",
    "    print(reader_cls)\n",
    "    if not name:\n",
    "        reader_instance = reader_cls(lazy=lazy, **kwargs)\n",
    "    else:\n",
    "        reader_instance = reader_cls(lazy=lazy, name=name, **kwargs)\n",
    "\n",
    "    datasets = reader_instance.read_datasets(data_files=data_files, splits=splits)\n",
    "    return datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(path_or_read_func,\n",
    "                 name=None,\n",
    "                 data_files=None,\n",
    "                 splits=None,\n",
    "                 lazy=None,\n",
    "                 **kwargs):\n",
    "   \n",
    "    reader_cls = CMRC2018\n",
    "    print(reader_cls)\n",
    "    if not name:\n",
    "        reader_instance = reader_cls(lazy=lazy, **kwargs)\n",
    "    else:\n",
    "        reader_instance = reader_cls(lazy=lazy, name=name, **kwargs)\n",
    "\n",
    "    datasets = reader_instance.read_datasets(data_files=data_files, splits=splits)\n",
    "    return datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, dev_ds,test_ds = load_dataset('cmrc2018', splits=('train', 'dev','test'))\n",
    "# 打印2条训练集\n",
    "for idx in range(2):\n",
    "    print(train_ds[idx]['question'])\n",
    "    print(train_ds[idx]['context'])\n",
    "    print(train_ds[idx]['answers'])\n",
    "    print(train_ds[idx]['answer_starts'])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 打印2条测试集\n",
    "for idx in range(2):\n",
    "    print(test_ds[idx]['question'])\n",
    "    print(test_ds[idx]['context'])\n",
    "    print(test_ds[idx]['answers'])\n",
    "    print(test_ds[idx]['answer_starts'])\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "罗亚尔港号是什么级别的导弹巡洋舰？\n",
    "罗亚尔港号（USS Port Royal CG-73）是美国海军提康德罗加级导弹巡洋舰，是该级巡洋舰的第27艘也是最后一艘。它也是美国海军第二艘以皇家港为名字命名的军舰。第一艘是1862年下水、曾参与南北战争的。船名来自曾在美国独立战争和南北战争中均发生过海战的南卡罗来纳州（Port Royal Sound）。美国海军在1988年2月25日订购该船，1991年10月18日在密西西比州帕斯卡古拉河畔的英戈尔斯造船厂放置龙骨。1992年11月20日下水，1992年12月5日由苏珊·贝克（Susan G. Baker，老布什政府时期的白宫办公厅主任，也是前国务卿詹姆斯·贝克的夫人）为其命名，1994年7月9日正式服役。2009年2月5日，罗亚尔港号巡洋舰在位于檀香山国际机场以南0.5英里的一处珊瑚礁上发生搁浅，之前该舰刚完成在旱坞内的维护，正在进行维护后的第一次海试。2009年2月9日凌晨2点，罗亚尔港号被脱离珊瑚礁。无人在这次事故中受伤，也未发生船上燃料的泄漏。但由于这次搁浅，罗亚尔港号巡洋舰不得不回到旱坞重新进行维修。1995年12月加入尼米兹号为核心的航空母舰战斗群，参与了南方守望行动，这是罗亚尔港号巡洋舰首次参与的军事部署行动。1996年3月由于台湾海峡导弹危机的发生被部署到了南中国海，随着危机的结束，1997年9月至1998年3月回到尼米兹号航空母舰战斗群参与南方守望行动。后随约翰·C·斯坦尼斯号航空母舰战斗群继续参加南方守望行动。2000年1月由于多次追击涉嫌违反联合国禁运制裁走私偷运伊拉克原油的船只因而造成对船上动力设备的持续性机械磨损而撤离，回到夏威夷进行整修和升级。2001年11月7日加入约翰·C·斯坦尼斯号航空母舰战斗群参与旨在对基地组织和对它进行庇护的阿富汗塔利班政权进行打击的持久自由军事行动。\n",
    "['FAKE_ANSWER_1', 'FAKE_ANSWER_2', 'FAKE_ANSWER_3']\n",
    "[-1, -1, -1]\n",
    "\n",
    "罗亚尔港号是美国第几艘以皇家港为名字命名的军舰？\n",
    "罗亚尔港号（USS Port Royal CG-73）是美国海军提康德罗加级导弹巡洋舰，是该级巡洋舰的第27艘也是最后一艘。它也是美国海军第二艘以皇家港为名字命名的军舰。第一艘是1862年下水、曾参与南北战争的。船名来自曾在美国独立战争和南北战争中均发生过海战的南卡罗来纳州（Port Royal Sound）。美国海军在1988年2月25日订购该船，1991年10月18日在密西西比州帕斯卡古拉河畔的英戈尔斯造船厂放置龙骨。1992年11月20日下水，1992年12月5日由苏珊·贝克（Susan G. Baker，老布什政府时期的白宫办公厅主任，也是前国务卿詹姆斯·贝克的夫人）为其命名，1994年7月9日正式服役。2009年2月5日，罗亚尔港号巡洋舰在位于檀香山国际机场以南0.5英里的一处珊瑚礁上发生搁浅，之前该舰刚完成在旱坞内的维护，正在进行维护后的第一次海试。2009年2月9日凌晨2点，罗亚尔港号被脱离珊瑚礁。无人在这次事故中受伤，也未发生船上燃料的泄漏。但由于这次搁浅，罗亚尔港号巡洋舰不得不回到旱坞重新进行维修。1995年12月加入尼米兹号为核心的航空母舰战斗群，参与了南方守望行动，这是罗亚尔港号巡洋舰首次参与的军事部署行动。1996年3月由于台湾海峡导弹危机的发生被部署到了南中国海，随着危机的结束，1997年9月至1998年3月回到尼米兹号航空母舰战斗群参与南方守望行动。后随约翰·C·斯坦尼斯号航空母舰战斗群继续参加南方守望行动。2000年1月由于多次追击涉嫌违反联合国禁运制裁走私偷运伊拉克原油的船只因而造成对船上动力设备的持续性机械磨损而撤离，回到夏威夷进行整修和升级。2001年11月7日加入约翰·C·斯坦尼斯号航空母舰战斗群参与旨在对基地组织和对它进行庇护的阿富汗塔利班政权进行打击的持久自由军事行动。\n",
    "['FAKE_ANSWER_1', 'FAKE_ANSWER_2', 'FAKE_ANSWER_3']\n",
    "[-1, -1, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"ernie-1.0\"\n",
    "if(MODEL_NAME==\"bert-base-chinese\"):\n",
    "    tokenizer = ppnlp.transformers.BertTokenizer.from_pretrained(MODEL_NAME)\n",
    "elif(MODEL_NAME==\"roberta-wwm-ext\"):\n",
    "    tokenizer=ppnlp.transformers.RobertaTokenizer.from_pretrained(MODEL_NAME)\n",
    "elif(MODEL_NAME==\"ernie-1.0\"):\n",
    "    tokenizer=ppnlp.transformers.ErnieTokenizer.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2022-01-24 20:11:12,587] [    INFO] - Downloading https://paddlenlp.bj.bcebos.com/models/transformers/ernie/vocab.txt and saved to /home/aistudio/.paddlenlp/models/ernie-1.0\n",
    "[2022-01-24 20:11:12,589] [    INFO] - Downloading vocab.txt from https://paddlenlp.bj.bcebos.com/models/transformers/ernie/vocab.txt\n",
    "100%|██████████| 90/90 [00:00<00:00, 2302.72it/s]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 512\n",
    "doc_stride = 128\n",
    "\n",
    "train_trans_func = partial(prepare_train_features, \n",
    "                           max_seq_length=max_seq_length, \n",
    "                           doc_stride=doc_stride,\n",
    "                           tokenizer=tokenizer)\n",
    "\n",
    "train_ds.map(train_trans_func, batched=True)\n",
    "\n",
    "dev_trans_func = partial(prepare_validation_features, \n",
    "                           max_seq_length=max_seq_length, \n",
    "                           doc_stride=doc_stride,\n",
    "                           tokenizer=tokenizer)\n",
    "                           \n",
    "dev_ds.map(dev_trans_func, batched=True)\n",
    "\n",
    "test_ds.map(dev_trans_func, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_batch_sampler = paddle.io.DistributedBatchSampler(\n",
    "        train_ds, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "train_batchify_fn = lambda samples, fn=Dict({\n",
    "    \"input_ids\": Pad(axis=0, pad_val=tokenizer.pad_token_id),\n",
    "    \"token_type_ids\": Pad(axis=0, pad_val=tokenizer.pad_token_type_id),\n",
    "    \"start_positions\": Stack(dtype=\"int64\"),\n",
    "    \"end_positions\": Stack(dtype=\"int64\")\n",
    "}): fn(samples)\n",
    "\n",
    "train_data_loader = paddle.io.DataLoader(\n",
    "    dataset=train_ds,\n",
    "    batch_sampler=train_batch_sampler,\n",
    "    collate_fn=train_batchify_fn,\n",
    "    return_list=True)\n",
    "\n",
    "dev_batch_sampler = paddle.io.BatchSampler(\n",
    "    dev_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "dev_batchify_fn = lambda samples, fn=Dict({\n",
    "    \"input_ids\": Pad(axis=0, pad_val=tokenizer.pad_token_id),\n",
    "    \"token_type_ids\": Pad(axis=0, pad_val=tokenizer.pad_token_type_id)\n",
    "}): fn(samples)\n",
    "\n",
    "test_batch_sampler = paddle.io.BatchSampler(\n",
    "    test_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "dev_data_loader = paddle.io.DataLoader(\n",
    "    dataset=dev_ds,\n",
    "    batch_sampler=dev_batch_sampler,\n",
    "    collate_fn=dev_batchify_fn,\n",
    "    return_list=True)\n",
    "\n",
    "test_data_loader = paddle.io.DataLoader(\n",
    "    dataset=test_ds,\n",
    "    batch_sampler=test_batch_sampler,\n",
    "    collate_fn=dev_batchify_fn,\n",
    "    return_list=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 设置想要使用模型的名称\n",
    "if(MODEL_NAME==\"bert-base-chinese\"):\n",
    "    model = ppnlp.transformers.BertForQuestionAnswering.from_pretrained(MODEL_NAME)\n",
    "elif(MODEL_NAME==\"roberta-wwm-ext\"):\n",
    "    model=ppnlp.transformers.RobertaForQuestionAnswering.from_pretrained(MODEL_NAME)\n",
    "elif(MODEL_NAME==\"ernie-1.0\"):\n",
    "    model=ppnlp.transformers.ErnieForQuestionAnswering.from_pretrained(MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[2022-01-24 20:13:15,116] [    INFO] - Downloading https://paddlenlp.bj.bcebos.com/models/transformers/ernie/ernie_v1_chn_base.pdparams and saved to /home/aistudio/.paddlenlp/models/ernie-1.0\n",
    "[2022-01-24 20:13:15,120] [    INFO] - Downloading ernie_v1_chn_base.pdparams from https://paddlenlp.bj.bcebos.com/models/transformers/ernie/ernie_v1_chn_base.pdparams\n",
    "100%|██████████| 392507/392507 [00:10<00:00, 38156.03it/s]\n",
    "W0124 20:13:25.550086  5796 device_context.cc:447] Please NOTE: device: 0, GPU Compute Capability: 7.0, Driver API Version: 10.1, Runtime API Version: 10.1\n",
    "W0124 20:13:25.558058  5796 device_context.cc:465] device: 0, cuDNN Version: 7.6.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossEntropyLossForSQuAD(paddle.nn.Layer):\n",
    "    def __init__(self):\n",
    "        super(CrossEntropyLossForSQuAD, self).__init__()\n",
    "\n",
    "    def forward(self, y, label):\n",
    "        start_logits, end_logits = y   # both shape are [batch_size, seq_len]\n",
    "        start_position, end_position = label\n",
    "        start_position = paddle.unsqueeze(start_position, axis=-1)\n",
    "        end_position = paddle.unsqueeze(end_position, axis=-1)\n",
    "        start_loss = paddle.nn.functional.softmax_with_cross_entropy(\n",
    "            logits=start_logits, label=start_position, soft_label=False)\n",
    "        start_loss = paddle.mean(start_loss)\n",
    "        end_loss = paddle.nn.functional.softmax_with_cross_entropy(\n",
    "            logits=end_logits, label=end_position, soft_label=False)\n",
    "        end_loss = paddle.mean(end_loss)\n",
    "\n",
    "        loss = (start_loss + end_loss) / 2\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 训练过程中的最大学习率\n",
    "learning_rate = 3e-5 \n",
    "# 训练轮次\n",
    "epochs = 3\n",
    "# 学习率预热比例\n",
    "warmup_proportion = 0.1\n",
    "# 权重衰减系数，类似模型正则项策略，避免模型过拟合\n",
    "weight_decay = 0.01\n",
    "\n",
    "num_training_steps = len(train_data_loader) * epochs\n",
    "lr_scheduler = ppnlp.transformers.LinearDecayWithWarmup(learning_rate, num_training_steps, warmup_proportion)\n",
    "\n",
    "# Generate parameter names needed to perform weight decay.\n",
    "# All bias and LayerNorm parameters are excluded.\n",
    "decay_params = [\n",
    "    p.name for n, p in model.named_parameters()\n",
    "    if not any(nd in n for nd in [\"bias\", \"norm\"])\n",
    "]\n",
    "optimizer = paddle.optimizer.AdamW(\n",
    "    learning_rate=lr_scheduler,\n",
    "    parameters=model.parameters(),\n",
    "    weight_decay=weight_decay,\n",
    "    apply_decay_param_fun=lambda x: x in decay_params)\n",
    "\n",
    "criterion = CrossEntropyLossForSQuAD()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@paddle.no_grad()\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "\n",
    "    all_start_logits = []\n",
    "    all_end_logits = []\n",
    "    tic_eval = time.time()\n",
    "\n",
    "    for batch in data_loader:\n",
    "        input_ids, token_type_ids = batch\n",
    "        start_logits_tensor, end_logits_tensor = model(input_ids,\n",
    "                                                       token_type_ids)\n",
    "\n",
    "        for idx in range(start_logits_tensor.shape[0]):\n",
    "            if len(all_start_logits) % 1000 == 0 and len(all_start_logits):\n",
    "                print(\"Processing example: %d\" % len(all_start_logits))\n",
    "                print('time per 1000:', time.time() - tic_eval)\n",
    "                tic_eval = time.time()\n",
    "\n",
    "            all_start_logits.append(start_logits_tensor.numpy()[idx])\n",
    "            all_end_logits.append(end_logits_tensor.numpy()[idx])\n",
    "\n",
    "    all_predictions, _, _ = compute_prediction(\n",
    "        data_loader.dataset.data, data_loader.dataset.new_data,\n",
    "        (all_start_logits, all_end_logits), False, 20, 30)\n",
    "    squad_evaluate(\n",
    "        examples=data_loader.dataset.data,\n",
    "        preds=all_predictions,\n",
    "        is_whitespace_splited=False)\n",
    "    \n",
    "    model.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "global_step = 0\n",
    "for epoch in range(1, epochs + 1):\n",
    "    for step, batch in enumerate(train_data_loader, start=1):\n",
    "        global_step += 1\n",
    "        input_ids, segment_ids, start_positions, end_positions = batch\n",
    "        logits = model(input_ids=input_ids, token_type_ids=segment_ids)\n",
    "        loss = criterion(logits, (start_positions, end_positions))\n",
    "\n",
    "        if global_step % 100 == 0 :\n",
    "            print(\"global step %d, epoch: %d, batch: %d, loss: %.5f\" % (global_step, epoch, step, loss))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.clear_grad()\n",
    "\n",
    "    evaluate(model=model, data_loader=dev_data_loader) \n",
    "\n",
    "model.save_pretrained('./checkpoint')\n",
    "tokenizer.save_pretrained('./checkpoint')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "global step 100, epoch: 1, batch: 100, loss: 2.56097\n",
    "global step 200, epoch: 1, batch: 200, loss: 2.41449\n",
    "global step 300, epoch: 1, batch: 300, loss: 1.83983\n",
    "global step 400, epoch: 1, batch: 400, loss: 1.65969\n",
    "global step 500, epoch: 1, batch: 500, loss: 1.88513\n",
    "Processing example: 1000\n",
    "time per 1000: 11.05217695236206\n",
    "Processing example: 2000\n",
    "time per 1000: 10.466019630432129\n",
    "Processing example: 3000\n",
    "time per 1000: 10.451993942260742\n",
    "Processing example: 4000\n",
    "time per 1000: 10.789400815963745\n",
    "Processing example: 5000\n",
    "time per 1000: 10.473366975784302\n",
    "{\n",
    "  \"exact\": 60.11183597390494,\n",
    "  \"f1\": 84.66431373532029,\n",
    "  \"total\": 3219,\n",
    "  \"HasAns_exact\": 60.11183597390494,\n",
    "  \"HasAns_f1\": 84.66431373532029,\n",
    "  \"HasAns_total\": 3219\n",
    "}\n",
    "global step 600, epoch: 2, batch: 68, loss: 0.89370\n",
    "global step 700, epoch: 2, batch: 168, loss: 1.15915\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@paddle.no_grad()\n",
    "def do_predict(model, data_loader):\n",
    "    model.eval()\n",
    "\n",
    "    all_start_logits = []\n",
    "    all_end_logits = []\n",
    "    tic_eval = time.time()\n",
    "\n",
    "    for batch in data_loader:\n",
    "        input_ids, token_type_ids = batch\n",
    "        start_logits_tensor, end_logits_tensor = model(input_ids,\n",
    "                                                       token_type_ids)\n",
    "\n",
    "        for idx in range(start_logits_tensor.shape[0]):\n",
    "            if len(all_start_logits) % 1000 == 0 and len(all_start_logits):\n",
    "                print(\"Processing example: %d\" % len(all_start_logits))\n",
    "                print('time per 1000:', time.time() - tic_eval)\n",
    "                tic_eval = time.time()\n",
    "\n",
    "            all_start_logits.append(start_logits_tensor.numpy()[idx])\n",
    "            all_end_logits.append(end_logits_tensor.numpy()[idx])\n",
    "\n",
    "    all_predictions, _, _ = compute_prediction(\n",
    "        data_loader.dataset.data, data_loader.dataset.new_data,\n",
    "        (all_start_logits, all_end_logits), False, 20, 30)\n",
    "\n",
    "\n",
    "    # Can also write all_nbest_json and scores_diff_json files if needed\n",
    "    with open('cmrc2018_predict.json', \"w\", encoding='utf-8') as writer:\n",
    "        writer.write(\n",
    "            json.dumps(\n",
    "                all_predictions, ensure_ascii=False, indent=4) + \"\\n\")\n",
    "    \n",
    "    model.train()\n",
    "do_predict(model, test_data_loader)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
